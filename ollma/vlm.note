┌─────────────────────────────────────────────────────────────────┐
│ Original Image    │  Patch Grid     │  Single Patch             │
│  (336×336)        │  (24×24 Grid).  │  (14×14)                  │
├─────────────────────────────────────────────────────────────────┤
│                   │  ┌─┬─┬─┬─┬─...  │  ┌──────────┐             │
│                   │  ├─┼─┼─┼─┼─...  │  │ Zoom in  │             │
│  🐱 on Sofa       │  │P0 ┼─┼─┼─...  │  │ P0 region│             │
│                   │  ├─┼─┼─┼─┼─...  │  │          │             │
│                   │  │ │ │P30┼─...  │  │          │             │
│                   │  ├─┼─┼─┼─┼─...  │  └──────────┘             │
│                   │  ... 24×24 ...  │                           │
└─────────────────────────────────────────────────────────────────┘

**Completed Patch Map：**
```
336×336 Image → 24×24 patches 
output: [576, 3, 14, 14] [number of patch, chennel, patch high, patch width]

line  0   1   2   3   4   5  ...  18  19  20  21  22  23
row┌────────────────────────────────────────────────────────┐
0  │  0   1   2   3   4   5  ...  18  19  20  21  22  23    │
1  │ 24  25  26  27  28  29  ...  42  43  44  45  46  47    │
2  │ 48  49  50  51  52  53  ...  66  67  68  69  70  71    │
3  │ 72  73  74  75  76  77  ...  90  91  92  93  94  95    │
...│ ...                                              ...   │
10 │240 241 242 243 244 245  ... 258 259 260 261 262 263    │
11 │264 265 266 267 268 269  ... 282 283 284 285 286 287    │
12 │288 289 290 291 292[300] ... 306 307 308 309 310 311    │ ← middle row
13 │312 313 314 315 316 317  ... 330 331 332 333 334 335    │
...│ ...                                              ...   │
23 │552 553 554 555 556 557  ... 570 571 572 573 574 575    │
   └────────────────────────────────────────────────────────┘
            ↑
          中心列

貓臉可能在: Patches 280-320 區域
貓身體可能在: Patches 320-400 區域
沙發可能在: Patches 400-550 區域

原始圖像 (1920×1080×3)
    ↓ Resize
調整大小 (336×336×3)
    ↓ Normalize
標準化 (336×336×3, 數值 ≈[-2, 2])
    ↓ Unfold / Conv2d
切分 patches (576 個 14×14 patches)
    ↓ Flatten
展平 (576, 588)
    ↓ 接下來...
Patch Embedding → Transformer → 語義理解
```

### **關鍵數字**
```
圖像大小: 336 × 336 pixels
Patch 大小: 14 × 14 pixels
Patches 數量: 24 × 24 = 576 個
每個 patch: 14 × 14 × 3 = 588 維
總數據量: 576 × 588 = 338,688 維
```

### **設計考量**
```
✓ Patch size 越小 → 更多細節，但計算慢
✓ Patch size 越大 → 計算快，但損失細節
✓ 14×14 是實踐中的最佳平衡點
✓ 圖像大小必須是 patch size 的整數倍

# 單個 patch 的 flatten
單個 patch: [3, 14, 14]

視覺化:
紅色通道 (14×14)      綠色通道 (14×14)      藍色通道 (14×14)
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│ 196 個數值   │      │ 196 個數值   │      │ 196 個數值   │
└─────────────┘      └─────────────┘      └─────────────┘
      ↓                   ↓                    ↓
展平後: [R1,R2,...,R196, G1,G2,...,G196, B1,B2,...,B196]
          └────────────── 588 個數值 ──────────────┘


步驟
階段 1️⃣：圖像處理 (Vision Pipeline)
【1-1】載入原始圖片
原始照片: cat_on_sofa.jpg
大小: (1920, 1080, 3)  # 寬×高×RGB
內容: 🐱坐在🛋️上

【1-2】預處理
操作: Resize + Normalize
├─ Resize: (1920, 1080) → (336, 336)
└─ Normalize: 像素值標準化到 [-2, 2]
輸出: (3, 336, 336)  # Channels×Height×Width

【1-3】切分 Patches
操作: 將圖片切成 14×14 的小格子
├─ Patch 大小: 14×14 pixels
├─ Patches 數量: (336÷14) × (336÷14) = 24×24 = 576
└─ 每個 patch: (3, 14, 14)
輸出: (576, 3, 14, 14)

【1-4】Flatten
操作: 將每個 patch 展平成一維向量
├─ 每個 patch: 3×14×14 = 588 個數值
└─ [R1,R2,...,R196, G1,G2,...,G196, B1,B2,...,B196]
輸出: (576, 588)  # 576個patches，每個588維
含義: 還是原始像素值，沒有語義

【1-5】Patch Embedding
操作: 線性投影，從像素空間 → 視覺語義空間
├─ Linear(588, 1024)
└─ 學習視覺特徵（顏色、紋理、形狀）
輸出: (576, 1024)
含義: 
  - Patch 0: [背景，淺色，平滑]
  - Patch 300: [毛茸茸，橙色，圓形] ← 貓臉
  - Patch 450: [布料，灰色，平面] ← 沙發

【1-6】Position Embedding
操作: 加入位置信息
├─ 每個 patch 加上 2D 位置編碼
└─ patch_embedding + position_embedding
輸出: (576, 1024)
含義: 現在知道「Patch 300 在圖片中心」

【1-7】加入 CLS Token
操作: 在開頭加入全局 token
├─ CLS Token: (1, 1024) - 隨機初始化
└─ [CLS] + [P0, P1, ..., P575]
輸出: (577, 1024)
含義: CLS 負責總結整張圖

【1-8】Vision Transformer (24層)
操作: 多層自注意力機制

Layer 0-5 (底層特徵):
  - 識別邊緣、顏色、紋理
  - Patch 300: "有圓形邊緣，橙棕色"

Layer 6-12 (中層特徵):
  - 組合成物體部件
  - Patches 145-168: "這些組成一隻眼睛"
  - Patches 280-320: "這些組成一張貓臉"
  
  Attention 示例:
  Patch 300 關注:
    → Patch 299, 301 (鄰近): 0.35
    → Patch 285-315 (貓臉區域): 0.45
    → 其他: 0.20

Layer 13-23 (高層語義):
  - 理解整體物體和關係
  - Patches 140-320: "這是一隻貓"
  - Patches 400-550: "這是沙發"
  - CLS Token 聚合: "貓坐在沙發上"
  
  CLS Attention 示例:
    → 貓區域 (140-320): 0.48
    → 沙發區域 (400-550): 0.32
    → 背景: 0.20

輸出: (577, 1024)
含義: 深度理解的視覺特徵
  - CLS: "整體場景：貓在沙發上"
  - Patch 300: "貓的臉部，包含眼睛鼻子"
  - Patch 450: "沙發的材質和顏色"

【1-9】Vision Encoder 輸出
輸出: (577, 1024)
├─ CLS Token: (1, 1024) - 全局理解
└─ Patch Tokens: (576, 1024) - 局部細節

階段 2️⃣：視覺-語言對齊 (Vision-Language Alignment)
python【2-1】Projection Layer
操作: 投影到 LLM 的語言空間
├─ 只投影 Patch Tokens (不投影 CLS)
├─ MLP: (1024) → GELU → (5120)
└─ 視覺空間 → 語言空間

輸入: (576, 1024)  # 視覺語義空間
輸出: (576, 5120)  # 語言語義空間

關鍵轉換:
視覺特徵 [毛茸茸, 橙色, 圓形]
    ↓ Projection
語言特徵 ≈ embedding("貓") + embedding("臉")

現在視覺 tokens 可以被 LLM 理解了！

階段 3️⃣：文字處理 (Text Pipeline)
python【3-1】構建 Prompt
用戶輸入: "沙發上是什麼動物"

VLM 格式化:
"""
USER: <image>
沙發上是什麼動物
ASSISTANT:
"""

【3-2】Tokenization
操作: 將文字切分成 tokens

Token 序列:
[<s>, USER, :, <image>, \n, 沙, 發, 上, 是, 什, 麼, 動, 物, \n, ASSISTANT, :]

Token IDs:
[1, 3148, 1001, 32000, 13, 27801, 30408, 29871, 30392, ...]

共 16 個 tokens

【3-3】Embedding
操作: 將 token IDs 轉成 embeddings
├─ 使用 LLM 的 embedding 層
└─ embed_tokens(token_ids)

輸出: (16, 5120)
每個 token 都是 5120 維向量

階段 4️⃣：多模態融合 (Multimodal Fusion)
python【4-1】找到 <image> 位置
<image> token 在位置 3

【4-2】組合序列
結構:
[<s>, USER, :] + [V1,V2,...,V576] + [\n, 沙, 發, ..., ASSISTANT, :]
   文字前         576個視覺tokens          文字後

拼接:
text_before:  (3, 5120)   # "<s>, USER, :"
visual_tokens: (576, 5120) # 圖像內容
text_after:   (13, 5120)  # 問題 + ASSISTANT

multimodal_sequence = concat(text_before, visual_tokens, text_after)

【4-3】最終輸入序列
輸出: (592, 5120)
    = 3 + 576 + 13

位置映射:
├─ 0-2:    文字 (<s>, USER, :)
├─ 3-578:  視覺 (圖像的 576 個 patches)
│   ├─ 3-26:     圖片第一行
│   ├─ 146-169:  貓的眼睛區域
│   ├─ 283-323:  貓的臉和身體
│   └─ 453-553:  沙發區域
└─ 579-591: 文字 (沙發上是什麼動物\nASSISTANT:)

階段 5️⃣：LLM 推理 (Language Model Inference)
python【5-1】進入 LLM Transformer (48層)
輸入: (592, 5120)

Layer 0-15 (理解階段):
  
  Layer 5:
  "沙發上" tokens 關注:
    → 視覺 V450-V550 (沙發區域): 0.55 ← 高！
    → 視覺 V280-V320 (貓區域): 0.25
    → 其他: 0.20
  
  理解: "問題在問沙發上的東西"
  
  Layer 10:
  "什麼動物" tokens 關注:
    → 視覺 V280-V320 (貓): 0.65 ← 非常高！
    → 視覺 V450-V550 (沙發): 0.20
    → 文字 "沙發上": 0.15
  
  理解: "需要識別貓這個動物"

Layer 16-30 (深度整合):
  
  Layer 20:
  "ASSISTANT" token 整合所有信息:
    → 視覺 (貓): 0.45
    → 視覺 (沙發): 0.25  
    → 文字問題: 0.30
  
  準備回答: "貓在沙發上"

Layer 31-47 (準備生成):
  
  最終理解:
  ✓ 問題問「沙發上」的「動物」
  ✓ 圖中沙發上有一隻貓
  ✓ 貓是動物
  ✓ 答案: 貓

【5-2】輸出 Hidden States
輸出: (592, 5120)
最後一個 token (位置 591) 的表示包含完整理解

階段 6️⃣：生成答案 (Text Generation)
python【6-1】取最後 Token
last_hidden = hidden_states[-1]  # (5120,)

【6-2】Final Layer Normalization
normalized = LayerNorm(last_hidden)  # (5120,)

【6-3】LM Head (投影到詞彙表)
logits = Linear(normalized)  # (32000,)
每個詞彙的「分數」

【6-4】Softmax (轉成機率)
probs = softmax(logits / temperature)

Top 10 候選:
  1. "沙" - 0.2845
  2. "貓" - 0.2134  ← 
  3. "一" - 0.1523
  4. "這" - 0.0876
  5. "是" - 0.0654
  ...

【6-5】Sampling (選擇 token)
使用 Top-P (p=0.9)
選中: "沙" (token_id: 27801)

【6-6】解碼
"沙" → 添加到輸出

【6-7】自回歸循環
當前輸出: "沙"

重複步驟 6-1 到 6-6:

循環 2: 生成 "發"
  當前: "沙發"
  
循環 3: 生成 "上"
  當前: "沙發上"
  
循環 4: 生成 "是"  
  當前: "沙發上是"
  
  此時關注:
  → 視覺 V280-V320 (貓): 0.70 ← 很高！
  → 已生成的文字: 0.20
  → 問題: 0.10

循環 5: 生成 "一"
  當前: "沙發上是一"

循環 6: 生成 "隻"
  當前: "沙發上是一隻"

循環 7: 生成 "貓"  ← 關鍵！
  當前: "沙發上是一隻貓"
  
  此時仍在參考視覺信息:
  → 視覺 V280-V320: 0.65
  → 確認是「貓」而不是「狗」

循環 8: 生成 "。"
  當前: "沙發上是一隻貓。"

【6-8】檢查停止條件
遇到句號 "。" → 停止生成

【6-9】最終輸出
"沙發上是一隻貓。"



完整流程圖
python輸入
├─ 圖片: 🐱 on 🛋️ (1920×1080)
└─ 文字: "沙發上是什麼動物"

↓

┌─────────────────────────────────────────┐
│ 階段 1: 圖像處理                         │
├─────────────────────────────────────────┤
│ (1920,1080,3)                           │
│   ↓ Resize + Normalize                  │
│ (3, 336, 336)                           │
│   ↓ Patch 切分                          │
│ (576, 3, 14, 14)                        │
│   ↓ Flatten                             │
│ (576, 588) ← 像素                       │
│   ↓ Patch Embedding                     │
│ (576, 1024) ← 視覺語義                  │
│   ↓ + Position + CLS                    │
│ (577, 1024)                             │
│   ↓ Vision Transformer (24層)          │
│ (577, 1024) ← 深度理解                  │
└─────────────────────────────────────────┘

↓

┌─────────────────────────────────────────┐
│ 階段 2: 視覺-語言對齊                    │
├─────────────────────────────────────────┤
│ (576, 1024) ← 視覺空間                  │
│   ↓ Projection Layer                    │
│ (576, 5120) ← 語言空間                  │
└─────────────────────────────────────────┘

↓

┌─────────────────────────────────────────┐
│ 階段 3: 文字處理                         │
├─────────────────────────────────────────┤
│ "沙發上是什麼動物"                       │
│   ↓ Tokenization                        │
│ [沙, 發, 上, 是, ...]                    │
│   ↓ Embedding                           │
│ (16, 5120)                              │
└─────────────────────────────────────────┘

↓

┌─────────────────────────────────────────┐
│ 階段 4: 多模態融合                       │
├─────────────────────────────────────────┤
│ 文字前 (3) + 視覺 (576) + 文字後 (13)    │
│   ↓ Concatenate                         │
│ (592, 5120) ← 混合序列                  │
└─────────────────────────────────────────┘

↓

┌─────────────────────────────────────────┐
│ 階段 5: LLM 推理                         │
├─────────────────────────────────────────┤
│ (592, 5120)                             │
│   ↓ LLM Transformer (48層)             │
│   • Cross-modal Attention              │
│   • 文字問題理解視覺內容                 │
│ (592, 5120)                             │
└─────────────────────────────────────────┘

↓

┌─────────────────────────────────────────┐
│ 階段 6: 生成答案                         │
├─────────────────────────────────────────┤
│ 取最後 token (5120)                     │
│   ↓ Layer Norm                          │
│   ↓ LM Head                             │
│ Logits (32000)                          │
│   ↓ Softmax                             │
│ Probs (32000)                           │
│   ↓ Sampling                            │
│ 下一個 token                             │
│   ↓ 自回歸循環                           │
│ "沙" → "發" → "上" → "是" → "一" →      │
│ "隻" → "貓" → "。"                      │
└─────────────────────────────────────────┘

↓

輸出: "沙發上是一隻貓。"

 關鍵數字總結
python圖像處理:
├─ 原始: 1920×1080×3 = 6,220,800 個像素
├─ Resize: 336×336×3 = 338,688 個像素
├─ Patches: 576 個 (24×24 網格)
├─ Flatten: 576×588 = 338,688 個數值
├─ Embedding: 576×1024 = 589,824 個參數
└─ Vision Encoder 輸出: 577×1024 = 590,848

投影:
└─ 576×5120 = 2,949,120

文字:
├─ Tokens: 16 個
└─ Embeddings: 16×5120 = 81,920

融合:
└─ 總序列: (3+576+13)×5120 = 592×5120 = 3,031,040

LLM 處理:
├─ 層數: 48 層
├─ 每層參數: ~10億
└─ 總參數: ~340億 (Vicuna-34B)

生成:
├─ 詞彙表大小: 32,000
├─ 生成 tokens: 8 個
└─ 輸出: "沙發上是一隻貓。"